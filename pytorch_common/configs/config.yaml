# These directories will typically get overridden
artifact_dir: ~/pytorch_common
checkpoint_dir: checkpoints
plot_dir: plots
log_dir: logs

# # Dataset config
# dataset_config:
#   size: 50
#   dim: &dim 10
#   num_classes: &num_classes 2

# Replace with your model name
model_name: single_layer_classifier

# Specify model type for model metrics tracker
model_type: classification

# Type of classification, if any
# Choices: binary | multiclass | multilabel
classification_type: binary

# # Model config
# model_config:
#   in_dim: *dim
#   num_classes: *num_classes

# Loss criterion
loss_criterion: cross-entropy
# loss_kwargs: # kwargs for loss criterion
  # reduction_train: "mean" # Reduction of training loss
  # reduction_val: "mean" # Reduction of val loss
  # multilabel_reduction: "mean" # Must be provided for multilabel
  # alpha: 0.7
  # gamma: 0.2

# Evaluation criteria
eval_criteria:
  - "accuracy"
  # - "precision"
  # - "recall"
  # - "f1"
  # - "auc"
early_stopping_criterion: "accuracy"
# eval_criteria_kwargs: # kwargs for eval_criteria
#     # multilabel_reduction: "mean" # Must be provided for multilabel
#     f1:
#       average: "macro"
#     precision:
#       average: "macro"
#     recall:
#       average: "macro"
#     auc:
#       pos_label: 1

# Whether to use scheduler (and where) and early stopping
use_scheduler_after_step: False
use_scheduler_after_epoch: False
use_early_stopping: False

# Disable saving/loading checkpoints (for faster dev)
disable_checkpointing: False

# Flag to ensure GPU is available for very big models
assert_gpu: False

# Training config
device: "cuda:0" # Default device

# # For parallelizing model.
# # If empty, will only use one GPU
# # If -1, will parallelize across all available GPUs
# # otherwise specify a list of GPU IDs
# device_ids: -1

seed: 0
